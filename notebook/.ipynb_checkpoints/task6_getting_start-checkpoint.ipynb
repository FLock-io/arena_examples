{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifan/anaconda3/envs/flock-training/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import yaml\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# working_dir = '/home/yifan/playground/arena_examples_code/notebook'\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "working_dir = os.path.dirname(notebook_dir)\n",
    "sys.path.append(working_dir)\n",
    "# used to load environment variables from the .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"FLOCK_API_KEY\"] = \"somekey\"\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA TITAN X (Pascal)\n",
      "  Total memory: 11.90 GB\n",
      "  CUDA Capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available with PyTorch\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {gpu_count}\")\n",
    "    \n",
    "    # Display information about each GPU\n",
    "    for i in range(gpu_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "        print(f\"  Total memory: {gpu_properties.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  CUDA Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will be slow on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Producer: SEEK48\n",
      "<p>FLock introduces SEEK48, an advanced AI idol producer model designed to help idol teams precisely select members, establish unique identities, and develop the most effective debut strategies. From image building and talent matching to fan engagement strategies, AI leverages data analysis and trend forecasting to create idol groups with strong market appeal. Whether shaping group concepts or optimizing individual career paths, our AI ensures that every idol takes the stage in the best possible way to become the next superstar.</p><br/><p>FLock 推出全新的 AI 偶像制作人模型 SEEK48，帮助偶像团队精准筛选成员、确定个性化定位，并制定最优出道方案。从形象塑造、才艺匹配到粉丝策略，AI 将结合数据分析与趋势预测，助力打造具有市场吸引力的偶像团队。无论是策划团体风格，还是优化个人发展路线，我们的AI都让每位偶像都能以最适合的方式站上舞台，成为未来的超级明星。</p><br/><p>FLock은 새로운 AI 아이돌 프로듀서 모델 SEEK48을 출시하여, 아이돌 팀이 멤버를 정교하게 선발하고 개성을 확립하며 최적의 데뷔 전략을 수립할 수 있도록 돕습니다. 이미지 구축부터 재능 매칭, 팬 전략까지, AI는 데이터 분석과 트렌드 예측을 결합하여 시장에서 경쟁력 있는 아이돌 그룹을 탄생시킵니다. 그룹 컨셉 기획부터 개인 성장 최적화까지, 우리의 AI는 각 아이돌이 자신의 강점을 극대화하여 무대에서 빛날 수 있도록 지원합니다.</p><br/><p>FLock は、最新の AI アイドルプロデューサーモデル SEEK48 を発表しました。このモデルは、アイドルチームがメンバーの選定や個性の確立、最適なデビュー戦略の策定を支援します。イメージ構築、才能のマッチング、ファン戦略まで、AI はデータ分析とトレンド予測を活用し、市場で成功するアイドルグループの形成をサポートします。グループのコンセプト設計から個人のキャリアプランの最適化まで、当社の AI は、すべてのアイドルが最も輝く形でステージに立てるよう支援します。</p>\n",
      "{'training_set_url': 'https://fed-ledger-prod-dataset.s3.amazonaws.com/6/training_set.jsonl?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIASSFQ745NLT5K57N2%2F20250321%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250321T005212Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=16cb6a377214af12177a98554070c1c2383fe1cbe29a11688772730536fb4361', 'max_params': 8000000000, 'context_length': 4096}\n",
      "\n",
      "Maximum parameters allowed: 8.0B parameters\n",
      "2025-03-30T23:59:59.975240\n",
      "2025-04-04T23:59:59.975240\n"
     ]
    }
   ],
   "source": [
    "# This cell fetches task information from the Flock API\n",
    "# It retrieves details about a specific task using its ID\n",
    "# The response includes title, description, data, and important dates\n",
    "\n",
    "task_id=\"6\"\n",
    "location = f'https://fed-ledger-prod.flock.io/api/v1/tasks/get?task_id={task_id}'\n",
    "response = requests.get(location)\n",
    "task = json.loads(response.text)\n",
    "data_url = task[\"data\"][\"training_set_url\"]\n",
    "print(task['title'])\n",
    "print(task['description'])\n",
    "print(task['data'])\n",
    "\n",
    "\n",
    "print()\n",
    "max_params = task['data']['max_params']\n",
    "if isinstance(max_params, int):\n",
    "    # Convert to billions and format\n",
    "    params_in_billions = max_params / 1_000_000_000\n",
    "    print(f\"Maximum parameters allowed: {params_in_billions:.1f}B parameters\")\n",
    "else:\n",
    "    print(max_params)\n",
    "\n",
    "print(task['submission_phase_ends_at'])\n",
    "print(task['final_validation_ends_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to /home/yifan/playground/arena_examples_code/data/task6_demo_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(data_url, stream=True)\n",
    "train_file = f\"{working_dir}/data/task{task_id}_demo_data.jsonl\"\n",
    "os.makedirs(os.path.dirname(train_file), exist_ok=True)\n",
    "with open(train_file, \"wb\") as f:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        if chunk:  # filter out keep-alive new chunks\n",
    "            f.write(chunk)\n",
    "    f.flush()\n",
    "    os.fsync(f.fileno())\n",
    "print(f\"Data saved successfully to {train_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random item from the dataset:\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"私たちのアイドルグループは、10人のメンバーで構成されており、全員が異なる特技を持っています。例えば、ダンスに特化したメンバーや歌唱力が高いメンバー、さらには演技経験があるメンバーもいます。今はトレーニング中ですが、デビューのための戦略を考えています。\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"それは素晴らしいですね！多様なスキルを持つメンバーがいることで、様々なパフォーマンスが可能になります。デビュー戦略としては、まずはそれぞれのメンバーの特性を活かしたユニークなスタイルを確立することが重要です。例えば、ダンスメンバーを前面に押し出したパフォーマンスや、歌の実力を強調したミュージックビデオを制作すると良いでしょう。\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"確かに、ステージパフォーマンスのスタイルを明確にすることは重要ですね。私たちの外見イメージや舞台コーディネートについても考えなければなりません。どのようにしてファンの心をつかむことができるでしょうか？\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"ファンの心をつかむためには、外見イメージをしっかりと設定することが必要です。各メンバーの個性を活かしたコーディネートを行い、全体としての統一感を持たせることが大切です。また、SNSを活用して、メンバーのキャラクター設定や日常の様子をファンに見せることで、親近感を持たせることも効果的です。特に、SNSの影響力を意識して、定期的にコンテンツを発信することが鍵になります。\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"SNSの活用は確かに効果的ですね。私たちのファン層については、若い世代が中心で、特にSNSを活用しているファンが多いです。彼らの活動度も高いですが、ブランドイメージをどう確立していくべきでしょうか？\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"若い世代のファン層をターゲットにするのであれば、彼らが共感できるメッセージやテーマを持つことが重要です。例えば、夢や努力をテーマにした楽曲やパフォーマンスを通じて、ファンとの感情的なつながりを強化します。また、ファンとのインタラクションを増やすために、定期的なオンラインイベントやQ&Aセッションを開催することも有効です。これにより、ファンの活動度をさらに高めることができるでしょう。\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"なるほど、ファンとのインタラクションを増やすことが大切なんですね。チームとしての連携も考えなければなりません。メンバー間のチームワークを強化するために、どのような活動が考えられますか？\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"メンバー間のチームワークを強化するためには、共同のトレーニングやレクリエーション活動が効果的です。例えば、定期的にチームビルディングのワークショップを行ったり、共通の趣味を持つメンバー同士でのアクティビティを企画すると良いでしょう。また、パフォーマンスのリハーサルを通じて、お互いの強みを理解しあう時間を持つことで、自然と連携が深まります。\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"チームビルディングの活動は良いアイデアですね。最後に、デビュー後の商業的ポテンシャルについてどう思いますか？特に、映画やドラマへの出演なども視野に入れた方が良いのでしょうか？\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"デビュー後の商業的ポテンシャルを考える際に、映画やドラマへの出演は非常に有意義です。これにより、メンバーそれぞれの個性を広くアピールできるだけでなく、ファン層を拡大するチャンスにもなります。また、バラエティ番組への出演もファンの感情をより深めることに繋がります。ターゲット市場とトレンドを分析しながら、戦略的に活動を展開することが重要です。\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Total number of items in the dataset: 90\n"
     ]
    }
   ],
   "source": [
    "# Load and display a random item from the downloaded JSONL file\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Read all lines from the JSONL file\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Select a random line\n",
    "random_line = random.choice(lines)\n",
    "\n",
    "# Parse the JSON\n",
    "random_item = json.loads(random_line)\n",
    "\n",
    "# Display the random item\n",
    "print(\"Random item from the dataset:\")\n",
    "print(json.dumps(random_item, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Print the number of items in the dataset\n",
    "print(f\"\\nTotal number of items in the dataset: {len(lines)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arg_file = f\"{working_dir}/args/task{task_id}_training_args.yaml\"\n",
    "with open(train_arg_file, 'r') as f:\n",
    "    all_training_args_as_list = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'per_device_train_batch_size': 1,\n",
       " 'gradient_accumulation_steps': 8,\n",
       " 'num_train_epochs': 1,\n",
       " 'lora_rank': 8,\n",
       " 'lora_alpha': 16,\n",
       " 'lora_dropout': 0.1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the model config that we are going to use\n",
    "use_args = all_training_args_as_list['Qwen/Qwen1.5-0.5B']\n",
    "# use_args['lora_rank'] = 4\n",
    "# use_args['lora_alpha'] = 8\n",
    "use_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_template = {\n",
    "    \"system_format\": \"<|im_start|>system\\n{content}<|im_end|>\\n\",\n",
    "    \"user_format\": \"<|im_start|>user\\n{content}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    \"assistant_format\": \"{content}<|im_end|>\\n\",\n",
    "    \"tool_format\": \"{content}\",\n",
    "    \"function_format\": \"{content}\",\n",
    "    \"observation_format\": \"<|im_start|>tool\\n{content}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    \"system\": \"You are a helpful assistant.\",\n",
    "}\n",
    "\n",
    "gemma_template = {\n",
    "    \"system_format\": \"<bos>\",\n",
    "    \"user_format\": \"<start_of_turn>user\\n{content}<end_of_turn>\\n<start_of_turn>model\\n\",\n",
    "    \"assistant_format\": \"{content}<eos>\\n\",\n",
    "    \"tool_format\": \"{content}\",\n",
    "    \"function_format\": \"{content}\",\n",
    "    \"observation_format\": \"<start_of_turn>tool\\n{content}<end_of_turn>\\n<start_of_turn>model\\n\",\n",
    "    \"system\": None,\n",
    "}\n",
    "\n",
    "model2template = {\n",
    "    \"Qwen/Qwen1.5-0.5B\": qwen_template,\n",
    "    \"Qwen/Qwen1.5-1.8B\": qwen_template,\n",
    "    \"Qwen/Qwen1.5-7B\": qwen_template,\n",
    "    \"google/gemma-2b\": gemma_template,\n",
    "    \"google/gemma-7b\": gemma_template,\n",
    "}\n",
    "\n",
    "model2size = {\n",
    "    \"Qwen/Qwen1.5-0.5B\": 620_000_000,\n",
    "    \"Qwen/Qwen1.5-1.8B\": 1_840_000_000,\n",
    "    \"Qwen/Qwen1.5-7B\": 7_720_000_000,\n",
    "    \"google/gemma-2b\": 2_510_000_000,\n",
    "    \"google/gemma-7b\": 8_540_000_000,\n",
    "}\n",
    "\n",
    "model2base_model = {\n",
    "    \"Qwen/Qwen1.5-0.5B\": \"qwen1.5\",\n",
    "    \"Qwen/Qwen1.5-1.8B\": \"qwen1.5\",\n",
    "    \"Qwen/Qwen1.5-7B\": \"qwen1.5\",\n",
    "    \"google/gemma-2b\": \"gemma\",\n",
    "    \"google/gemma-7b\": \"gemma\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, file, tokenizer, max_seq_length, template):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system_format = template[\"system_format\"]\n",
    "        self.user_format = template[\"user_format\"]\n",
    "        self.assistant_format = template[\"assistant_format\"]\n",
    "        self.tool_format = template[\"tool_format\"]\n",
    "        self.function_format = template[\"function_format\"]\n",
    "        self.observation_format = template[\"observation_format\"]\n",
    "\n",
    "        self.max_seq_length = max_seq_length\n",
    "        # logger.info(\"Loading data: {}\".format(file))\n",
    "        with open(file, \"r\", encoding=\"utf8\") as f:\n",
    "            data_list = f.readlines()\n",
    "        # logger.info(\"There are {} data in dataset\".format(len(data_list)))\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data_list[index]\n",
    "        data = json.loads(data)\n",
    "        input_ids, target_mask = [], []\n",
    "\n",
    "        # setting system information\n",
    "        if self.system_format is not None:\n",
    "            system = data[\"system\"].strip() if \"system\" in data.keys() else self.system\n",
    "\n",
    "            if system is not None:\n",
    "                system_text = self.system_format.format(content=system)\n",
    "                input_ids = self.tokenizer.encode(system_text, add_special_tokens=False)\n",
    "                target_mask = [0] * len(input_ids)\n",
    "\n",
    "        conversations = data[\"conversations\"]\n",
    "\n",
    "        input_buffer = \"\"\n",
    "        for i in range(len(conversations)):\n",
    "            role = conversations[i][\"role\"]\n",
    "            content = conversations[i][\"content\"].strip()\n",
    "\n",
    "            if role != \"assistant\":\n",
    "                if role == \"user\":\n",
    "                    human = self.user_format.format(\n",
    "                        content=content, stop_token=self.tokenizer.eos_token\n",
    "                    )\n",
    "                    input_buffer += human\n",
    "\n",
    "            else:\n",
    "                assistant = self.assistant_format.format(\n",
    "                    content=content, stop_token=self.tokenizer.eos_token\n",
    "                )\n",
    "\n",
    "                input_tokens = self.tokenizer.encode(\n",
    "                    input_buffer, add_special_tokens=False\n",
    "                )\n",
    "                output_tokens = self.tokenizer.encode(\n",
    "                    assistant, add_special_tokens=False\n",
    "                )\n",
    "\n",
    "                input_ids += input_tokens + output_tokens\n",
    "                target_mask += [0] * len(input_tokens) + [1] * len(output_tokens)\n",
    "                input_buffer = \"\"\n",
    "\n",
    "        assert len(input_ids) == len(target_mask)\n",
    "\n",
    "        input_ids = input_ids[: self.max_seq_length]\n",
    "        target_mask = target_mask[: self.max_seq_length]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        assert len(input_ids) == len(target_mask) == len(attention_mask)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"target_mask\": target_mask,\n",
    "        }\n",
    "        return inputs\n",
    "\n",
    "\n",
    "\n",
    "class SFTDataCollator(object):\n",
    "    def __init__(self, tokenizer, max_seq_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        # Find the maximum length in the batch\n",
    "        lengths = [len(x[\"input_ids\"]) for x in batch if x[\"input_ids\"] is not None]\n",
    "        # Take the maximum length in the batch, if it exceeds max_seq_length, take max_seq_length\n",
    "        batch_max_len = min(max(lengths), self.max_seq_length)\n",
    "\n",
    "        input_ids_batch, attention_mask_batch, target_mask_batch = [], [], []\n",
    "        # Truncate and pad\n",
    "        for x in batch:\n",
    "            input_ids = x[\"input_ids\"]\n",
    "            attention_mask = x[\"attention_mask\"]\n",
    "            target_mask = x[\"target_mask\"]\n",
    "            if input_ids is None:\n",
    "                logger.info(\"some input_ids is None\")\n",
    "                continue\n",
    "            padding_len = batch_max_len - len(input_ids)\n",
    "            # Pad\n",
    "            input_ids = input_ids + [self.pad_token_id] * padding_len\n",
    "            attention_mask = attention_mask + [0] * padding_len\n",
    "            target_mask = target_mask + [0] * padding_len\n",
    "            # Truncate\n",
    "            input_ids = input_ids[: self.max_seq_length]\n",
    "            attention_mask = attention_mask[: self.max_seq_length]\n",
    "            target_mask = target_mask[: self.max_seq_length]\n",
    "\n",
    "            input_ids_batch.append(input_ids)\n",
    "            attention_mask_batch.append(attention_mask)\n",
    "            target_mask_batch.append(target_mask)\n",
    "\n",
    "        # Convert lists to tensors to get the final model input\n",
    "        input_ids_batch = torch.tensor(input_ids_batch, dtype=torch.long)\n",
    "        attention_mask_batch = torch.tensor(attention_mask_batch, dtype=torch.long)\n",
    "        target_mask_batch = torch.tensor(target_mask_batch, dtype=torch.long)\n",
    "        # input_ids_batch = torch.tensor(input_ids_batch, dtype=torch.long, device='cuda:0')\n",
    "        # attention_mask_batch = torch.tensor(attention_mask_batch, dtype=torch.long, device='cuda:0')\n",
    "        # target_mask_batch = torch.tensor(target_mask_batch, dtype=torch.long, device='cuda:0')\n",
    "\n",
    "        labels = torch.where(target_mask_batch == 1, input_ids_batch, -100)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids_batch,\n",
    "            \"attention_mask\": attention_mask_batch,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LoraTrainingArguments:\n",
    "    per_device_train_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    num_train_epochs: int\n",
    "    lora_rank: int\n",
    "    lora_alpha: int\n",
    "    lora_dropout: int\n",
    "\n",
    "def train_lora(\n",
    "    model_id: str, context_length: int, training_args: LoraTrainingArguments,\n",
    "    data_file_path: str,\n",
    "    model_output_dir: str,\n",
    "    model_template: dict,\n",
    "    target_module: list = [\"q_proj\", \"v_proj\"],\n",
    "    max_steps: int = None,  # New parameter to limit training steps\n",
    "):\n",
    "    assert model_id in model2template, f\"model_id {model_id} not supported\"\n",
    "    lora_config = LoraConfig(\n",
    "        r=training_args.lora_rank,\n",
    "        target_modules=target_module,\n",
    "        lora_alpha=training_args.lora_alpha,\n",
    "        lora_dropout=training_args.lora_dropout,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    # Load model in 4-bit to do qLoRA\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Configure training with option to limit steps\n",
    "    train_config = {\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"bf16\": True,\n",
    "        \"logging_steps\": 20,\n",
    "        \"output_dir\": \"outputs\",\n",
    "        \"optim\": \"paged_adamw_8bit\",\n",
    "        \"remove_unused_columns\": False,\n",
    "        \"max_seq_length\": context_length,\n",
    "    }\n",
    "    \n",
    "    # Either use max_steps or num_train_epochs\n",
    "    if max_steps is not None:\n",
    "        train_config[\"max_steps\"] = max_steps\n",
    "    else:\n",
    "        train_config[\"num_train_epochs\"] = training_args.num_train_epochs\n",
    "        \n",
    "    training_args = SFTConfig(**train_config)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        token=os.environ[\"HF_TOKEN\"],\n",
    "    )\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = SFTDataset(\n",
    "        file=data_file_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=context_length,\n",
    "        template=model_template,\n",
    "    )\n",
    "\n",
    "    # Define trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=training_args,\n",
    "        peft_config=lora_config,\n",
    "        data_collator=SFTDataCollator(tokenizer, max_seq_length=context_length),\n",
    "    )\n",
    "\n",
    "    # Train model with OOM handling\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except (RuntimeError, torch.cuda.OutOfMemoryError) as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"Caught OOM error. Saving current model state...\")\n",
    "        else:\n",
    "            print(f\"Error during training: {e}\")\n",
    "        # Save whatever progress was made before the error\n",
    "    \n",
    "    # Save model regardless of whether training completed or was interrupted\n",
    "    try:\n",
    "        trainer.save_model(model_output_dir)\n",
    "        print(f\"Model saved to {model_output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "    # remove checkpoint folder\n",
    "    os.system(\"rm -rf outputs/checkpoint-*\")\n",
    "\n",
    "    # upload lora weights and tokenizer\n",
    "    print(\"Training Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_template = model2template['Qwen/Qwen1.5-0.5B']\n",
    "use_template['system_format'] = None\n",
    "use_template['system'] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train the model Qwen/Qwen1.5-0.5B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:04, Epoch 0.18/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught OOM error. Saving current model state...\n",
      "Model saved to /home/yifan/playground/arena_examples_code/outputs/task6_Qwen/Qwen1.5-0.5B\n",
      "Training Completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "no_submission = True\n",
    "target_module = [\"q_proj\", \"v_proj\"] # default\n",
    "max_params = task[\"data\"][\"max_params\"]\n",
    "context_length = task[\"data\"][\"context_length\"]\n",
    "\n",
    "model_id = list(all_training_args_as_list.keys())[0]\n",
    "output_dir = f\"{working_dir}/outputs/task{task_id}_{model_id}\"\n",
    "\n",
    "print(f\"Start to train the model {model_id}...\")\n",
    "try:\n",
    "    train_lora(\n",
    "        model_id=model_id,\n",
    "        context_length=context_length,\n",
    "        training_args=LoraTrainingArguments(**use_args),\n",
    "        data_file_path=train_file,\n",
    "        model_output_dir=output_dir,\n",
    "        target_module=target_module,\n",
    "        model_template=use_template,\n",
    "        max_steps=3\n",
    "    )\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Proceed to the next model...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:flock-training]",
   "language": "python",
   "name": "conda-env-flock-training-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
