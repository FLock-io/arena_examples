{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import yaml\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "working_dir = os.path.dirname(notebook_dir)\n",
    "sys.path.append(working_dir)\n",
    "\n",
    "\n",
    "# used to load environment variables from the .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"FLOCK_API_KEY\"] = \"somekey\"\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n",
      "Number of available GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "  Total memory: 23.64 GB\n",
      "  CUDA Capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available with PyTorch\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {gpu_count}\")\n",
    "    \n",
    "    # Display information about each GPU\n",
    "    for i in range(gpu_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "        print(f\"  Total memory: {gpu_properties.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  CUDA Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will be slow on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen SKYST Datathon: AI Hedge Fund\n",
      "\n",
      "{'training_set_url': 'https://flock-fl-param.s3.amazonaws.com/36/training_set.jsonl?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIASSFQ745NHQLBLUN4%2F20250730%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250730T021004Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb089a00751b7ab3de8a451bfeebd37b0b4001d8d086abd3388901968f39d894', 'max_params': 8000000000, 'context_length': 4096}\n",
      "\n",
      "Maximum parameters allowed: 8.0B parameters\n",
      "2025-08-01T23:59:59.634879\n",
      "2025-08-06T23:59:59.634879\n"
     ]
    }
   ],
   "source": [
    "# This cell fetches task information from the Flock API\n",
    "# It retrieves details about a specific task using its ID\n",
    "# The response includes title, description, data, and important dates\n",
    "\n",
    "task_id=\"36\"\n",
    "location = f'https://fed-ledger-staging.flock.io/api/v1/tasks/get?task_id={task_id}'\n",
    "response = requests.get(location)\n",
    "task = json.loads(response.text)\n",
    "data_url = task[\"data\"][\"training_set_url\"]\n",
    "print(task['title'])\n",
    "print(task['description'])\n",
    "print(task['data'])\n",
    "\n",
    "\n",
    "print()\n",
    "max_params = task['data']['max_params']\n",
    "if isinstance(max_params, int):\n",
    "    # Convert to billions and format\n",
    "    params_in_billions = max_params / 1_000_000_000\n",
    "    print(f\"Maximum parameters allowed: {params_in_billions:.1f}B parameters\")\n",
    "else:\n",
    "    print(max_params)\n",
    "\n",
    "print(task['submission_phase_ends_at'])\n",
    "print(task['final_validation_ends_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to /workspace/arena_examples/data/skyst_task36_demo_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(data_url, stream=True)\n",
    "train_file = f\"{working_dir}/data/skyst_task{task_id}_demo_data.jsonl\"\n",
    "os.makedirs(os.path.dirname(train_file), exist_ok=True)\n",
    "with open(train_file, \"wb\") as f:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        if chunk:  # filter out keep-alive new chunks\n",
    "            f.write(chunk)\n",
    "    f.flush()\n",
    "    os.fsync(f.fileno())\n",
    "print(f\"Data saved successfully to {train_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random item from the dataset:\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Given Amazon's Q4 2022 revenue of $574,785,000,000, up from $513,983,000,000 in Q4 2021 (11.8% growth), a cost of revenue of $480,980,000,000, a gross profit of $93,805,000,000 (16.3% margin), an operating income of $36,852,000,000, a net income of $30,425,000,000, and the traditional strength of Q4 due to the holiday shopping season, can you provide the latest quarterly revenue figures for Amazon?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"Amazon's Q4 2022 performance showcases significant revenue growth driven by strong market demand and effective operational management. The 11.8% year-over-year increase highlights the company's ability to capitalize on the holiday shopping season, which consistently boosts sales. Despite high operational costs, Amazon maintained a healthy gross margin and achieved substantial operating and net income, demonstrating efficient cost management and profitability. These results reinforce Amazon's dominant position in the e-commerce market and reflect positively on its strategic investments in areas like cloud computing and logistics infrastructure, positioning the company for sustained growth and market leadership.\"\n",
      "    }\n",
      "  ],\n",
      "  \"system\": \"You are an expert in the financial field and are good at answering financial-related questions raised by everyone.\"\n",
      "}\n",
      "\n",
      "Total number of items in the dataset: 228\n"
     ]
    }
   ],
   "source": [
    "# Load and display a random item from the downloaded JSONL file\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Read all lines from the JSONL file\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Select a random line\n",
    "random_line = random.choice(lines)\n",
    "\n",
    "# Parse the JSON\n",
    "random_item = json.loads(random_line)\n",
    "\n",
    "# Display the random item\n",
    "print(\"Random item from the dataset:\")\n",
    "print(json.dumps(random_item, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Print the number of items in the dataset\n",
    "print(f\"\\nTotal number of items in the dataset: {len(lines)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arg_file = f\"{working_dir}/args/skyst_task{task_id}_training_args.yaml\"\n",
    "with open(train_arg_file, 'r') as f:\n",
    "    all_training_args_as_list = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining several utility dicts that will be used by the fine-tuning process\n",
    "\n",
    "qwen_template = {\n",
    "    \"system_format\": \"<|im_start|>system\\n{content}<|im_end|>\\n\",\n",
    "    \"user_format\": \"<|im_start|>user\\n{content}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    \"assistant_format\": \"{content}<|im_end|>\\n\",\n",
    "    \"tool_format\": \"{content}\",\n",
    "    \"function_format\": \"{content}\",\n",
    "    \"observation_format\": \"<|im_start|>tool\\n{content}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    \"system\": \"You are a helpful assistant.\",\n",
    "}\n",
    "\n",
    "gemma_template = {\n",
    "    \"system_format\": \"<bos>\",\n",
    "    \"user_format\": \"<start_of_turn>user\\n{content}<end_of_turn>\\n<start_of_turn>model\\n\",\n",
    "    \"assistant_format\": \"{content}<eos>\\n\",\n",
    "    \"tool_format\": \"{content}\",\n",
    "    \"function_format\": \"{content}\",\n",
    "    \"observation_format\": \"<start_of_turn>tool\\n{content}<end_of_turn>\\n<start_of_turn>model\\n\",\n",
    "    \"system\": None,\n",
    "}\n",
    "\n",
    "model2template = {\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\": qwen_template, \n",
    "    \"Qwen/Qwen2.5-3B-Instruct\": qwen_template,    \n",
    "    \"Qwen/Qwen1.5-0.5B\": qwen_template,\n",
    "    \"Qwen/Qwen1.5-1.8B\": qwen_template,\n",
    "    \"Qwen/Qwen1.5-7B\": qwen_template,\n",
    "}\n",
    "\n",
    "model2size = {\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\": 7_620_000_000, \n",
    "    \"Qwen/Qwen2.5-3B-Instruct\": 3_090_000_000,\n",
    "    \"Qwen/Qwen1.5-0.5B\": 620_000_000,\n",
    "    \"Qwen/Qwen1.5-1.8B\": 1_840_000_000,\n",
    "    \"Qwen/Qwen1.5-7B\": 7_720_000_000,\n",
    "}\n",
    "\n",
    "model2base_model = {\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\": \"qwen1.5\", # just use qwen1.5 here, it's the same model family \n",
    "    \"Qwen/Qwen2.5-3B-Instruct\": \"qwen1.5\",\n",
    "    \"Qwen/Qwen1.5-0.5B\": \"qwen1.5\",\n",
    "    \"Qwen/Qwen1.5-1.8B\": \"qwen1.5\",\n",
    "    \"Qwen/Qwen1.5-7B\": \"qwen1.5\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, file, tokenizer, max_seq_length, template):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system_format = template[\"system_format\"]\n",
    "        self.user_format = template[\"user_format\"]\n",
    "        self.assistant_format = template[\"assistant_format\"]\n",
    "        self.tool_format = template[\"tool_format\"]\n",
    "        self.function_format = template[\"function_format\"]\n",
    "        self.observation_format = template[\"observation_format\"]\n",
    "\n",
    "        self.max_seq_length = max_seq_length\n",
    "        # logger.info(\"Loading data: {}\".format(file))\n",
    "        with open(file, \"r\", encoding=\"utf8\") as f:\n",
    "            data_list = f.readlines()\n",
    "        # logger.info(\"There are {} data in dataset\".format(len(data_list)))\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data_list[index]\n",
    "        data = json.loads(data)\n",
    "        input_ids, target_mask = [], []\n",
    "\n",
    "        # setting system information\n",
    "        if self.system_format is not None:\n",
    "            system = data[\"system\"].strip() if \"system\" in data.keys() else self.system\n",
    "\n",
    "            if system is not None:\n",
    "                system_text = self.system_format.format(content=system)\n",
    "                input_ids = self.tokenizer.encode(system_text, add_special_tokens=False)\n",
    "                target_mask = [0] * len(input_ids)\n",
    "\n",
    "        conversations = data[\"conversations\"]\n",
    "\n",
    "        input_buffer = \"\"\n",
    "        for i in range(len(conversations)):\n",
    "            role = conversations[i][\"role\"]\n",
    "            content = conversations[i][\"content\"].strip()\n",
    "\n",
    "            if role != \"assistant\":\n",
    "                if role == \"user\":\n",
    "                    human = self.user_format.format(\n",
    "                        content=content, stop_token=self.tokenizer.eos_token\n",
    "                    )\n",
    "                    input_buffer += human\n",
    "\n",
    "            else:\n",
    "                assistant = self.assistant_format.format(\n",
    "                    content=content, stop_token=self.tokenizer.eos_token\n",
    "                )\n",
    "\n",
    "                input_tokens = self.tokenizer.encode(\n",
    "                    input_buffer, add_special_tokens=False\n",
    "                )\n",
    "                output_tokens = self.tokenizer.encode(\n",
    "                    assistant, add_special_tokens=False\n",
    "                )\n",
    "\n",
    "                input_ids += input_tokens + output_tokens\n",
    "                target_mask += [0] * len(input_tokens) + [1] * len(output_tokens)\n",
    "                input_buffer = \"\"\n",
    "\n",
    "        assert len(input_ids) == len(target_mask)\n",
    "\n",
    "        input_ids = input_ids[: self.max_seq_length]\n",
    "        target_mask = target_mask[: self.max_seq_length]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        assert len(input_ids) == len(target_mask) == len(attention_mask)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"target_mask\": target_mask,\n",
    "        }\n",
    "        return inputs\n",
    "\n",
    "\n",
    "\n",
    "class SFTDataCollator(object):\n",
    "    def __init__(self, tokenizer, max_seq_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        # Find the maximum length in the batch\n",
    "        lengths = [len(x[\"input_ids\"]) for x in batch if x[\"input_ids\"] is not None]\n",
    "        # Take the maximum length in the batch, if it exceeds max_seq_length, take max_seq_length\n",
    "        batch_max_len = min(max(lengths), self.max_seq_length)\n",
    "\n",
    "        input_ids_batch, attention_mask_batch, target_mask_batch = [], [], []\n",
    "        # Truncate and pad\n",
    "        for x in batch:\n",
    "            input_ids = x[\"input_ids\"]\n",
    "            attention_mask = x[\"attention_mask\"]\n",
    "            target_mask = x[\"target_mask\"]\n",
    "            if input_ids is None:\n",
    "                logger.info(\"some input_ids is None\")\n",
    "                continue\n",
    "            padding_len = batch_max_len - len(input_ids)\n",
    "            # Pad\n",
    "            input_ids = input_ids + [self.pad_token_id] * padding_len\n",
    "            attention_mask = attention_mask + [0] * padding_len\n",
    "            target_mask = target_mask + [0] * padding_len\n",
    "            # Truncate\n",
    "            input_ids = input_ids[: self.max_seq_length]\n",
    "            attention_mask = attention_mask[: self.max_seq_length]\n",
    "            target_mask = target_mask[: self.max_seq_length]\n",
    "\n",
    "            input_ids_batch.append(input_ids)\n",
    "            attention_mask_batch.append(attention_mask)\n",
    "            target_mask_batch.append(target_mask)\n",
    "\n",
    "        # Convert lists to tensors to get the final model input\n",
    "        input_ids_batch = torch.tensor(input_ids_batch, dtype=torch.long)\n",
    "        attention_mask_batch = torch.tensor(attention_mask_batch, dtype=torch.long)\n",
    "        target_mask_batch = torch.tensor(target_mask_batch, dtype=torch.long)\n",
    "        # input_ids_batch = torch.tensor(input_ids_batch, dtype=torch.long, device='cuda:0')\n",
    "        # attention_mask_batch = torch.tensor(attention_mask_batch, dtype=torch.long, device='cuda:0')\n",
    "        # target_mask_batch = torch.tensor(target_mask_batch, dtype=torch.long, device='cuda:0')\n",
    "\n",
    "        labels = torch.where(target_mask_batch == 1, input_ids_batch, -100)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids_batch,\n",
    "            \"attention_mask\": attention_mask_batch,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LoraTrainingArguments:\n",
    "    per_device_train_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    num_train_epochs: int\n",
    "    lora_rank: int\n",
    "    lora_alpha: int\n",
    "    lora_dropout: int\n",
    "\n",
    "def train_lora(\n",
    "    model_id: str, context_length: int, training_args: LoraTrainingArguments,\n",
    "    data_file_path: str,\n",
    "    model_output_dir: str,\n",
    "    model_template: dict,\n",
    "    target_module: list = [\"q_proj\", \"v_proj\"],\n",
    "    max_steps: int = None,  # New parameter to limit training steps\n",
    "):\n",
    "    assert model_id in model2template, f\"model_id {model_id} not supported\"\n",
    "    lora_config = LoraConfig(\n",
    "        r=training_args.lora_rank,\n",
    "        target_modules=target_module,\n",
    "        lora_alpha=training_args.lora_alpha,\n",
    "        lora_dropout=training_args.lora_dropout,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    # Load model in 4-bit to do qLoRA\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Configure training with option to limit steps\n",
    "    train_config = {\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"bf16\": True,\n",
    "        \"logging_steps\": 20,\n",
    "        \"output_dir\": \"outputs\",\n",
    "        \"optim\": \"paged_adamw_8bit\",\n",
    "        \"remove_unused_columns\": False,\n",
    "        \"max_seq_length\": context_length,\n",
    "    }\n",
    "    \n",
    "    # Either use max_steps or num_train_epochs\n",
    "    if max_steps is not None:\n",
    "        train_config[\"max_steps\"] = max_steps\n",
    "    else:\n",
    "        train_config[\"num_train_epochs\"] = training_args.num_train_epochs\n",
    "        \n",
    "    training_args = SFTConfig(**train_config)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        token=os.environ[\"HF_TOKEN\"],\n",
    "    )\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = SFTDataset(\n",
    "        file=data_file_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=context_length,\n",
    "        template=model_template,\n",
    "    )\n",
    "\n",
    "    # Define trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=training_args,\n",
    "        peft_config=lora_config,\n",
    "        data_collator=SFTDataCollator(tokenizer, max_seq_length=context_length),\n",
    "    )\n",
    "\n",
    "    # Train model with OOM handling\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except (RuntimeError, torch.cuda.OutOfMemoryError) as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"Caught OOM error. Saving current model state...\")\n",
    "        else:\n",
    "            print(f\"Error during training: {e}\")\n",
    "        # Save whatever progress was made before the error\n",
    "    \n",
    "    # Save model regardless of whether training completed or was interrupted\n",
    "    try:\n",
    "        trainer.save_model(model_output_dir)\n",
    "        print(f\"Model saved to {model_output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "    # remove checkpoint folder\n",
    "    os.system(\"rm -rf outputs/checkpoint-*\")\n",
    "\n",
    "    # upload lora weights and tokenizer\n",
    "    print(\"Training Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_template = model2template['Qwen/Qwen2.5-3B-Instruct']\n",
    "\n",
    "# in some tasks, the system prompt is not needed - we need this for task 36\n",
    "# use_template['system_format'] = None\n",
    "# use_template['system'] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen2.5-3B-Instruct\n",
      "{'per_device_train_batch_size': 1, 'gradient_accumulation_steps': 8, 'num_train_epochs': 1, 'lora_rank': 8, 'lora_alpha': 16, 'lora_dropout': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# take a look at the model config that we are going to use\n",
    "use_model_id = list(all_training_args_as_list.keys())[1]\n",
    "use_args = all_training_args_as_list[use_model_id]\n",
    "# use_args['lora_rank'] = 4\n",
    "# use_args['lora_alpha'] = 8\n",
    "print(use_model_id)\n",
    "print(use_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train the model Qwen/Qwen2.5-3B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005980014801025391,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "config.json",
       "rate": null,
       "total": 661,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d20ed14468e48eeb1392e9631bcc21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00881648063659668,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model.safetensors.index.json",
       "rate": null,
       "total": null,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a982d2094b4a19a469d954c66a0447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007716178894042969,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c013151ce11c412394b82981dfb3d9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008135557174682617,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model-00001-of-00002.safetensors",
       "rate": null,
       "total": 3968658944,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c631722f6d5a40a9805284b0db6b2717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008188486099243164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "model-00002-of-00002.safetensors",
       "rate": null,
       "total": 2203268048,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d728783b71e43ac9e21e488717b24a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004259347915649414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5788a4cda2f84cb8bfc63833afc68c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00455474853515625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "generation_config.json",
       "rate": null,
       "total": 242,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2d78d2f3f243e4b9d13e08406e79b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:413: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /workspace/arena_examples/outputs/task36_Qwen/Qwen2.5-3B-Instruct\n",
      "Training Completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "no_submission = True\n",
    "target_module = [\"q_proj\", \"v_proj\"] # default\n",
    "max_params = task[\"data\"][\"max_params\"]\n",
    "context_length = task[\"data\"][\"context_length\"]\n",
    "\n",
    "\n",
    "output_dir = f\"{working_dir}/outputs/task{task_id}_{use_model_id}\"\n",
    "\n",
    "print(f\"Start to train the model {use_model_id}...\")\n",
    "try:\n",
    "    train_lora(\n",
    "        model_id=use_model_id,\n",
    "        context_length=context_length,\n",
    "        training_args=LoraTrainingArguments(**use_args),\n",
    "        data_file_path=train_file,\n",
    "        model_output_dir=output_dir,\n",
    "        target_module=target_module,\n",
    "        model_template=use_template,\n",
    "        max_steps=3 # setting this to 3 for demonstration purpose, you might want to remove this to enable longer training\n",
    "    )\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Proceed to the next model...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example code for uploading model to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from huggingface_hub import HfApi, create_repo, ModelCard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2.5-3B-Instruct\n",
      "/workspace/arena_examples/outputs/task36_Qwen/Qwen2.5-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "model_id = use_model_id.split('/')[-1]\n",
    "model_dir = output_dir\n",
    "print(model_id)\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please set up HF_TOKEN and HF_USERNAME accordingly \n",
    "\n",
    "# Set up HuggingFace API with your token\n",
    "api = HfApi(token=os.getenv('HF_TOKEN'))\n",
    "\n",
    "# Define your own repository name\n",
    "my_repo_name = f\"{os.environ['HF_USERNAME']}/{model_id}\"  # Change this to your preferred name\n",
    "\n",
    "# Create a new repository (private by default)\n",
    "print(f\"Creating repository: {my_repo_name}\")\n",
    "create_repo(my_repo_name, private=False, token=os.getenv('HF_TOKEN'))\n",
    "\n",
    "# Upload all files from the local directory to your HF repository\n",
    "print(f\"Uploading files from {model_dir} to {my_repo_name}...\")\n",
    "api.upload_folder(\n",
    "    folder_path=model_dir,\n",
    "    repo_id=my_repo_name,\n",
    "    repo_type=\"model\",\n",
    "    ignore_patterns=[\"*.git*\", \"*.ipynb_checkpoints*\"]\n",
    ")\n",
    "\n",
    "print(f\"Successfully uploaded repository to https://huggingface.co/{my_repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# repo submission example\n",
    "Note - not directly executable, please modify accordingly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "apikey = os.environ[\"FLOCK_API_KEY\"]\n",
    "location = 'https://fed-ledger-staging.flock.io/api/v1/tasks/submit-result'\n",
    "\n",
    "repo = \"your huggingface repo path with your uploaded model\" \n",
    "base_model = \"qwen1.5\"\n",
    "revision = \"your model commit hash\" #find this in your huggingface model page\n",
    "\n",
    "header = {\n",
    "        \"flock-api-key\":apikey,\n",
    "        \"'Content-Type\": \"application/json\",\n",
    "}\n",
    "data = {\n",
    "    \"task_id\":os.environ[\"TASK_ID\"],\n",
    "    \"data\":{\n",
    "        \"hg_repo_id\": repo, \n",
    "        \"base_model\": base_model,\n",
    "        \"revision\": revision\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(location, headers=header, json=data)\n",
    "res = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
