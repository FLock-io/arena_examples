{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/training-node/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import yaml\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "working_dir = os.path.dirname(notebook_dir)\n",
    "sys.path.append(working_dir)\n",
    "# used to load environment variables from the .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"FLOCK_API_KEY\"] = \"somekey\"\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "1. Checking for GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: False\n",
      "No GPU available. Training will be slow on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available with PyTorch\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {gpu_count}\")\n",
    "    \n",
    "    # Display information about each GPU\n",
    "    for i in range(gpu_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "        print(f\"  Total memory: {gpu_properties.total_memory / 1024**3:.2f} GB\")\n",
    "        print(f\"  CUDA Capability: {gpu_properties.major}.{gpu_properties.minor}\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will be slow on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "2. Fetching Task Data from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Producer: SEEK48\n",
      "<p>FLock introduces SEEK48, an advanced AI idol producer model designed to help idol teams precisely select members, establish unique identities, and develop the most effective debut strategies. From image building and talent matching to fan engagement strategies, AI leverages data analysis and trend forecasting to create idol groups with strong market appeal. Whether shaping group concepts or optimizing individual career paths, our AI ensures that every idol takes the stage in the best possible way to become the next superstar.</p><br/><p>FLock 推出全新的 AI 偶像制作人模型 SEEK48，帮助偶像团队精准筛选成员、确定个性化定位，并制定最优出道方案。从形象塑造、才艺匹配到粉丝策略，AI 将结合数据分析与趋势预测，助力打造具有市场吸引力的偶像团队。无论是策划团体风格，还是优化个人发展路线，我们的AI都让每位偶像都能以最适合的方式站上舞台，成为未来的超级明星。</p><br/><p>FLock은 새로운 AI 아이돌 프로듀서 모델 SEEK48을 출시하여, 아이돌 팀이 멤버를 정교하게 선발하고 개성을 확립하며 최적의 데뷔 전략을 수립할 수 있도록 돕습니다. 이미지 구축부터 재능 매칭, 팬 전략까지, AI는 데이터 분석과 트렌드 예측을 결합하여 시장에서 경쟁력 있는 아이돌 그룹을 탄생시킵니다. 그룹 컨셉 기획부터 개인 성장 최적화까지, 우리의 AI는 각 아이돌이 자신의 강점을 극대화하여 무대에서 빛날 수 있도록 지원합니다.</p><br/><p>FLock は、最新の AI アイドルプロデューサーモデル SEEK48 を発表しました。このモデルは、アイドルチームがメンバーの選定や個性の確立、最適なデビュー戦略の策定を支援します。イメージ構築、才能のマッチング、ファン戦略まで、AI はデータ分析とトレンド予測を活用し、市場で成功するアイドルグループの形成をサポートします。グループのコンセプト設計から個人のキャリアプランの最適化まで、当社の AI は、すべてのアイドルが最も輝く形でステージに立てるよう支援します。</p>\n",
      "{'training_set_url': 'https://fed-ledger-prod-dataset.s3.amazonaws.com/6/training_set.jsonl?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIASSFQ745NLT5K57N2%2F20250321%2Fus-east-2%2Fs3%2Faws4_request&X-Amz-Date=20250321T212928Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=f40459fe63a3303e48af94116702437e414af3f9af115000ca7178839cf588c2', 'max_params': 8000000000, 'context_length': 4096}\n",
      "\n",
      "Maximum parameters allowed: 8.0B parameters\n",
      "2025-03-30T23:59:59.975240\n",
      "2025-04-04T23:59:59.975240\n"
     ]
    }
   ],
   "source": [
    "# This cell fetches task information from the Flock API\n",
    "# It retrieves details about a specific task using its ID\n",
    "# The response includes title, description, data, and important dates\n",
    "\n",
    "task_id=\"6\"\n",
    "location = f'https://fed-ledger-prod.flock.io/api/v1/tasks/get?task_id={task_id}'\n",
    "response = requests.get(location)\n",
    "task = json.loads(response.text)\n",
    "data_url = task[\"data\"][\"training_set_url\"]\n",
    "print(task['title'])\n",
    "print(task['description'])\n",
    "print(task['data'])\n",
    "\n",
    "\n",
    "print()\n",
    "max_params = task['data']['max_params']\n",
    "if isinstance(max_params, int):\n",
    "    # Convert to billions and format\n",
    "    params_in_billions = max_params / 1_000_000_000\n",
    "    print(f\"Maximum parameters allowed: {params_in_billions:.1f}B parameters\")\n",
    "else:\n",
    "    print(max_params)\n",
    "\n",
    "print(task['submission_phase_ends_at'])\n",
    "print(task['final_validation_ends_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "3. Downloading the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully to /Users/elizabethlui/work/arena_examples/data/task6_demo_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(data_url, stream=True)\n",
    "train_file = f\"{working_dir}/data/task{task_id}_demo_data.jsonl\"\n",
    "os.makedirs(os.path.dirname(train_file), exist_ok=True)\n",
    "with open(train_file, \"wb\") as f:\n",
    "    for chunk in response.iter_content(chunk_size=8192):\n",
    "        if chunk:  # filter out keep-alive new chunks\n",
    "            f.write(chunk)\n",
    "    f.flush()\n",
    "    os.fsync(f.fileno())\n",
    "print(f\"Data saved successfully to {train_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "4. Inspecting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random item from the dataset:\n",
      "{\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"我最近关注到一支新出道的偶像团体，他们团队共有5人。经过多年训练，团队内有人在唱功、rap、主持和创作方面都有不错的专长，另一些成员在形象管理上也很有优势，尤其在时尚品味和妆容风格上颇具市场吸引力。不过，目前这支团体的粉丝数量和活跃度还偏低，虽然公司在团队建设和资源投入上有相应的努力，但市场竞争压力仍然较大。请问你怎么看这支团体目前的优势与劣势？\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"这支团体的优势在于成员技能多元，能够覆盖唱功、rap、主持及创作等多个领域，这为出道后的多元化发展提供了坚实基础。另外，团队在形象管理方面表现出色，时尚与妆容细节都符合当前年轻市场的审美需求，可以在宣传和视觉效果上形成鲜明特色。劣势在于粉丝基础尚不够牢固，活跃度和忠诚度相对较低，这使得短期内专辑销量及综艺曝光度可能遇到瓶颈；同时，市场上已经有不少成熟的偶像团体，竞争较为激烈。总体来看，需要在粉丝运营和品牌塑造上下更多功夫。\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"针对之前的分析，我想更详细地讨论下出道后推广策略问题。比如在影视综艺资源的争取和专辑销量提升方面，你有什么切实可行的方案？另外，我也关注团队内在的凝聚力，如何在保持个性特色的同时增强整体合作呢？\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"在推广策略上，建议团队先制定一个清晰的品牌定位，利用成员各自不同的技能点打造‘多面手’形象。例如，可以为专注rap和创作的成员争取参与音乐剧或原创音乐节目的机会，而具备主持和唱功的成员则可争取参加热门综艺节目，从而切入影视和综艺资源。同时，配合线上社交媒体和粉丝活动互动，借助平台经济提升专辑销量。关于增强团队凝聚力，一方面可以组织内部定期的团建活动和创作工作坊，让成员在共同学习和磨合中形成默契；另一方面，公司也可以考虑设计团队联名的活动或专题节目，既突出个人才华又展示团队整体魅力，强化团队整体协调发展。\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"明白了。那对于长远发展，如何利用成员各自的个人特质，比如运动能力、时尚感和创作力，来推动跨领域发展，使整个团队在娱乐市场中长期保持竞争力？\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"长远来看，可以从以下几个方面发力：首先，针对成员的个人特质，制定个性化成长路线。例如，运动能力突出的成员可以尝试参与体育类综艺或品牌代言，将其健康形象和活力优势转化为商业价值；具备时尚品味的成员可以与潮流品牌合作，甚至参与时装周等活动，提升团队在潮流圈的影响力；而拥有良好创作能力的成员则可以推动原创音乐项目，甚至尝试跨领域的艺术创作，形成独特的音乐风格。其次，公司应当考虑打造一个灵活的市场策略，既保持团队整体统一的品牌形象，又允许个体在各自领域有更多发挥空间，从而实现‘联名效应’。这样既能吸引不同领域的粉丝，也能在市场上树立起独树一帜的跨界形象，为团队赢得长期竞争优势。\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Total number of items in the dataset: 90\n"
     ]
    }
   ],
   "source": [
    "# Load and display a random item from the downloaded JSONL file\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Read all lines from the JSONL file\n",
    "with open(train_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Select a random line\n",
    "random_line = random.choice(lines)\n",
    "\n",
    "# Parse the JSON\n",
    "random_item = json.loads(random_line)\n",
    "\n",
    "# Display the random item\n",
    "print(\"Random item from the dataset:\")\n",
    "print(json.dumps(random_item, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Print the number of items in the dataset\n",
    "print(f\"\\nTotal number of items in the dataset: {len(lines)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "5. Loading Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arg_file = f\"{working_dir}/args/task{task_id}_training_args.yaml\"\n",
    "with open(train_arg_file, 'r') as f:\n",
    "    all_training_args_as_list = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'per_device_train_batch_size': 1,\n",
       " 'gradient_accumulation_steps': 8,\n",
       " 'num_train_epochs': 1,\n",
       " 'lora_rank': 8,\n",
       " 'lora_alpha': 16,\n",
       " 'lora_dropout': 0.1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the model config that we are going to use\n",
    "use_args = all_training_args_as_list['Qwen/Qwen1.5-0.5B']\n",
    "# use_args['lora_rank'] = 4\n",
    "# use_args['lora_alpha'] = 8\n",
    "use_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_template = {\n",
    "    \"system_format\": \"<|im_start|>system\\n{content}<|im_end|>\\n\",\n",
    "    \"user_format\": \"<|im_start|>user\\n{content}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    \"assistant_format\": \"{content}<|im_end|>\\n\",\n",
    "    \"tool_format\": \"{content}\",\n",
    "    \"function_format\": \"{content}\",\n",
    "    \"observation_format\": \"<|im_start|>tool\\n{content}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "    \"system\": \"You are a helpful assistant.\",\n",
    "}\n",
    "\n",
    "gemma_template = {\n",
    "    \"system_format\": \"<bos>\",\n",
    "    \"user_format\": \"<start_of_turn>user\\n{content}<end_of_turn>\\n<start_of_turn>model\\n\",\n",
    "    \"assistant_format\": \"{content}<eos>\\n\",\n",
    "    \"tool_format\": \"{content}\",\n",
    "    \"function_format\": \"{content}\",\n",
    "    \"observation_format\": \"<start_of_turn>tool\\n{content}<end_of_turn>\\n<start_of_turn>model\\n\",\n",
    "    \"system\": None,\n",
    "}\n",
    "\n",
    "model2template = {\n",
    "    \"Qwen/Qwen1.5-0.5B\": qwen_template,\n",
    "    \"Qwen/Qwen1.5-1.8B\": qwen_template,\n",
    "    \"Qwen/Qwen1.5-7B\": qwen_template,\n",
    "    \"google/gemma-2b\": gemma_template,\n",
    "    \"google/gemma-7b\": gemma_template,\n",
    "}\n",
    "\n",
    "model2size = {\n",
    "    \"Qwen/Qwen1.5-0.5B\": 620_000_000,\n",
    "    \"Qwen/Qwen1.5-1.8B\": 1_840_000_000,\n",
    "    \"Qwen/Qwen1.5-7B\": 7_720_000_000,\n",
    "    \"google/gemma-2b\": 2_510_000_000,\n",
    "    \"google/gemma-7b\": 8_540_000_000,\n",
    "}\n",
    "\n",
    "model2base_model = {\n",
    "    \"Qwen/Qwen1.5-0.5B\": \"qwen1.5\",\n",
    "    \"Qwen/Qwen1.5-1.8B\": \"qwen1.5\",\n",
    "    \"Qwen/Qwen1.5-7B\": \"qwen1.5\",\n",
    "    \"google/gemma-2b\": \"gemma\",\n",
    "    \"google/gemma-7b\": \"gemma\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "6. Preparing datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, file, tokenizer, max_seq_length, template):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system_format = template[\"system_format\"]\n",
    "        self.user_format = template[\"user_format\"]\n",
    "        self.assistant_format = template[\"assistant_format\"]\n",
    "        self.tool_format = template[\"tool_format\"]\n",
    "        self.function_format = template[\"function_format\"]\n",
    "        self.observation_format = template[\"observation_format\"]\n",
    "\n",
    "        self.max_seq_length = max_seq_length\n",
    "        # logger.info(\"Loading data: {}\".format(file))\n",
    "        with open(file, \"r\", encoding=\"utf8\") as f:\n",
    "            data_list = f.readlines()\n",
    "        # logger.info(\"There are {} data in dataset\".format(len(data_list)))\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data_list[index]\n",
    "        data = json.loads(data)\n",
    "        input_ids, target_mask = [], []\n",
    "\n",
    "        # setting system information\n",
    "        if self.system_format is not None:\n",
    "            system = data[\"system\"].strip() if \"system\" in data.keys() else self.system\n",
    "\n",
    "            if system is not None:\n",
    "                system_text = self.system_format.format(content=system)\n",
    "                input_ids = self.tokenizer.encode(system_text, add_special_tokens=False)\n",
    "                target_mask = [0] * len(input_ids)\n",
    "\n",
    "        conversations = data[\"conversations\"]\n",
    "\n",
    "        input_buffer = \"\"\n",
    "        for i in range(len(conversations)):\n",
    "            role = conversations[i][\"role\"]\n",
    "            content = conversations[i][\"content\"].strip()\n",
    "\n",
    "            if role != \"assistant\":\n",
    "                if role == \"user\":\n",
    "                    human = self.user_format.format(\n",
    "                        content=content, stop_token=self.tokenizer.eos_token\n",
    "                    )\n",
    "                    input_buffer += human\n",
    "\n",
    "            else:\n",
    "                assistant = self.assistant_format.format(\n",
    "                    content=content, stop_token=self.tokenizer.eos_token\n",
    "                )\n",
    "\n",
    "                input_tokens = self.tokenizer.encode(\n",
    "                    input_buffer, add_special_tokens=False\n",
    "                )\n",
    "                output_tokens = self.tokenizer.encode(\n",
    "                    assistant, add_special_tokens=False\n",
    "                )\n",
    "\n",
    "                input_ids += input_tokens + output_tokens\n",
    "                target_mask += [0] * len(input_tokens) + [1] * len(output_tokens)\n",
    "                input_buffer = \"\"\n",
    "\n",
    "        assert len(input_ids) == len(target_mask)\n",
    "\n",
    "        input_ids = input_ids[: self.max_seq_length]\n",
    "        target_mask = target_mask[: self.max_seq_length]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        assert len(input_ids) == len(target_mask) == len(attention_mask)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"target_mask\": target_mask,\n",
    "        }\n",
    "        return inputs\n",
    "\n",
    "\n",
    "\n",
    "class SFTDataCollator(object):\n",
    "    def __init__(self, tokenizer, max_seq_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        # Find the maximum length in the batch\n",
    "        lengths = [len(x[\"input_ids\"]) for x in batch if x[\"input_ids\"] is not None]\n",
    "        # Take the maximum length in the batch, if it exceeds max_seq_length, take max_seq_length\n",
    "        batch_max_len = min(max(lengths), self.max_seq_length)\n",
    "\n",
    "        input_ids_batch, attention_mask_batch, target_mask_batch = [], [], []\n",
    "        # Truncate and pad\n",
    "        for x in batch:\n",
    "            input_ids = x[\"input_ids\"]\n",
    "            attention_mask = x[\"attention_mask\"]\n",
    "            target_mask = x[\"target_mask\"]\n",
    "            if input_ids is None:\n",
    "                logger.info(\"some input_ids is None\")\n",
    "                continue\n",
    "            padding_len = batch_max_len - len(input_ids)\n",
    "            # Pad\n",
    "            input_ids = input_ids + [self.pad_token_id] * padding_len\n",
    "            attention_mask = attention_mask + [0] * padding_len\n",
    "            target_mask = target_mask + [0] * padding_len\n",
    "            # Truncate\n",
    "            input_ids = input_ids[: self.max_seq_length]\n",
    "            attention_mask = attention_mask[: self.max_seq_length]\n",
    "            target_mask = target_mask[: self.max_seq_length]\n",
    "\n",
    "            input_ids_batch.append(input_ids)\n",
    "            attention_mask_batch.append(attention_mask)\n",
    "            target_mask_batch.append(target_mask)\n",
    "\n",
    "        # Convert lists to tensors to get the final model input\n",
    "        input_ids_batch = torch.tensor(input_ids_batch, dtype=torch.long)\n",
    "        attention_mask_batch = torch.tensor(attention_mask_batch, dtype=torch.long)\n",
    "        target_mask_batch = torch.tensor(target_mask_batch, dtype=torch.long)\n",
    "        # input_ids_batch = torch.tensor(input_ids_batch, dtype=torch.long, device='cuda:0')\n",
    "        # attention_mask_batch = torch.tensor(attention_mask_batch, dtype=torch.long, device='cuda:0')\n",
    "        # target_mask_batch = torch.tensor(target_mask_batch, dtype=torch.long, device='cuda:0')\n",
    "\n",
    "        labels = torch.where(target_mask_batch == 1, input_ids_batch, -100)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids_batch,\n",
    "            \"attention_mask\": attention_mask_batch,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LoraTrainingArguments:\n",
    "    per_device_train_batch_size: int\n",
    "    gradient_accumulation_steps: int\n",
    "    num_train_epochs: int\n",
    "    lora_rank: int\n",
    "    lora_alpha: int\n",
    "    lora_dropout: int\n",
    "\n",
    "def train_lora(\n",
    "    model_id: str, context_length: int, training_args: LoraTrainingArguments,\n",
    "    data_file_path: str,\n",
    "    model_output_dir: str,\n",
    "    model_template: dict,\n",
    "    target_module: list = [\"q_proj\", \"v_proj\"],\n",
    "    max_steps: int = None,  # New parameter to limit training steps\n",
    "):\n",
    "    assert model_id in model2template, f\"model_id {model_id} not supported\"\n",
    "    lora_config = LoraConfig(\n",
    "        r=training_args.lora_rank,\n",
    "        target_modules=target_module,\n",
    "        lora_alpha=training_args.lora_alpha,\n",
    "        lora_dropout=training_args.lora_dropout,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    # Load model in 4-bit to do qLoRA\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    \n",
    "    # Configure training with option to limit steps\n",
    "    train_config = {\n",
    "        \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n",
    "        \"gradient_accumulation_steps\": training_args.gradient_accumulation_steps,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"bf16\": True,\n",
    "        \"logging_steps\": 20,\n",
    "        \"output_dir\": \"outputs\",\n",
    "        # Change THIS line:\n",
    "        # \"optim\": \"paged_adamw_8bit\",  # bitsandbytes 8-bit\n",
    "        \"optim\": \"adamw_torch_fused\",   # a normal CPU/MPS-friendly optimizer\n",
    "        \"remove_unused_columns\": False,\n",
    "        \"max_seq_length\": context_length,\n",
    "    }\n",
    "    \n",
    "    # Either use max_steps or num_train_epochs\n",
    "    if max_steps is not None:\n",
    "        train_config[\"max_steps\"] = max_steps\n",
    "    else:\n",
    "        train_config[\"num_train_epochs\"] = training_args.num_train_epochs\n",
    "        \n",
    "    training_args = SFTConfig(**train_config)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        # quantization_config=bnb_config,\n",
    "        token=os.environ[\"HF_TOKEN\"],\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = SFTDataset(\n",
    "        file=data_file_path,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=context_length,\n",
    "        template=model_template,\n",
    "    )\n",
    "\n",
    "    # Define trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        args=training_args,\n",
    "        peft_config=lora_config,\n",
    "        data_collator=SFTDataCollator(tokenizer, max_seq_length=context_length),\n",
    "    )\n",
    "\n",
    "    # Train model with OOM handling\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except (RuntimeError, torch.cuda.OutOfMemoryError) as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(\"Caught OOM error. Saving current model state...\")\n",
    "        else:\n",
    "            print(f\"Error during training: {e}\")\n",
    "        # Save whatever progress was made before the error\n",
    "    \n",
    "    # Save model regardless of whether training completed or was interrupted\n",
    "    try:\n",
    "        trainer.save_model(model_output_dir)\n",
    "        print(f\"Model saved to {model_output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "    # remove checkpoint folder\n",
    "    os.system(\"rm -rf outputs/checkpoint-*\")\n",
    "\n",
    "    # upload lora weights and tokenizer\n",
    "    print(\"Training Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_template = model2template['Qwen/Qwen1.5-0.5B']\n",
    "use_template['system_format'] = None\n",
    "use_template['system'] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "7. Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to train the model Qwen/Qwen1.5-0.5B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/training-node/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "100%|██████████| 3/3 [00:24<00:00,  8.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.9792, 'train_samples_per_second': 0.961, 'train_steps_per_second': 0.12, 'train_loss': 2.6741841634114585, 'epoch': 0.27}\n",
      "Model saved to /Users/elizabethlui/work/arena_examples/outputs/task6_Qwen/Qwen1.5-0.5B\n",
      "Training Completed.\n"
     ]
    }
   ],
   "source": [
    "no_submission = True # set to False to submit the model; need API key to do so\n",
    "target_module = [\"q_proj\", \"v_proj\"] # default\n",
    "max_params = task[\"data\"][\"max_params\"]\n",
    "context_length = task[\"data\"][\"context_length\"]\n",
    "\n",
    "model_id = list(all_training_args_as_list.keys())[0]\n",
    "output_dir = f\"{working_dir}/outputs/task{task_id}_{model_id}\"\n",
    "\n",
    "print(f\"Start to train the model {model_id}...\")\n",
    "try:\n",
    "    train_lora(\n",
    "        model_id=model_id,\n",
    "        context_length=context_length,\n",
    "        training_args=LoraTrainingArguments(**use_args),\n",
    "        data_file_path=train_file,\n",
    "        model_output_dir=output_dir,\n",
    "        target_module=target_module,\n",
    "        model_template=use_template,\n",
    "        max_steps=3\n",
    "    )\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Proceed to the next model...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training-node",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
